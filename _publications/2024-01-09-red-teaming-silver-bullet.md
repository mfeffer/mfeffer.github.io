---
title: "Red-Teaming for Generative AI: Silver Bullet or Security Theater?"
collection: publications
permalink: /publication/2024-01-09-red-teaming-silver-bullet
excerpt: 'In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of the relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation).'
date: 2024-01-09
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2401.15897'
citation: 'Feffer, M., Sinha, A., Lipton, Z. C., &amp; Heidari, H. (2024). Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv (2024)'
---

<a href='https://arxiv.org/abs/2401.15897'>Download paper here</a>

In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of the relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation).

Recommended citation: Feffer, M., Sinha, A., Lipton, Z. C., & Heidari, H. (2024). Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv (2024)